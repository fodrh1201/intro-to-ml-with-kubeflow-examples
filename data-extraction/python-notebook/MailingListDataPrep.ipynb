{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.6/site-packages (4.4.1)\n",
      "\u001b[31mError checking for conflicts.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2897, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2691, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2888, in _parsed_pkg_info\n",
      "    return self._pkg_info\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2691, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _pkg_info\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 503, in _warn_about_conflicts\n",
      "    package_set, _dep_info = check_install_conflicts(to_install)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/operations/check.py\", line 108, in check_install_conflicts\n",
      "    package_set, _ = create_package_set_from_installed()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/operations/check.py\", line 47, in create_package_set_from_installed\n",
      "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2635, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2899, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2908, in _compute_dependencies\n",
      "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2890, in _parsed_pkg_info\n",
      "    metadata = self.get_metadata(self.PKG_INFO)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1410, in get_metadata\n",
      "    value = self._get(self._fn(self.egg_info, name))\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1522, in _get\n",
      "    with open(path, 'rb') as stream:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/jovyan/.local/lib/python3.6/site-packages/kfp-0.1.32.dist-info/METADATA'\u001b[0m\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting git+https://github.com/kubernetes-client/python.git\n",
      "  Cloning https://github.com/kubernetes-client/python.git to /tmp/pip-req-build-7vdumt5f\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.6/site-packages (from kubernetes===10.0.0-snapshot) (2019.3.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from kubernetes===10.0.0-snapshot) (1.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.6/site-packages (from kubernetes===10.0.0-snapshot) (2.8.0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.6/site-packages (from kubernetes===10.0.0-snapshot) (40.9.0)\n",
      "Requirement already satisfied: pyyaml>=3.12 in /opt/conda/lib/python3.6/site-packages (from kubernetes===10.0.0-snapshot) (5.1)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from kubernetes===10.0.0-snapshot) (1.6.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.6/site-packages (from kubernetes===10.0.0-snapshot) (0.56.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from kubernetes===10.0.0-snapshot) (2.22.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.6/site-packages (from kubernetes===10.0.0-snapshot) (1.2.0)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.6/site-packages (from kubernetes===10.0.0-snapshot) (1.24.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.0.1->kubernetes===10.0.0-snapshot) (0.2.4)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.0.1->kubernetes===10.0.0-snapshot) (4.0)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.0.1->kubernetes===10.0.0-snapshot) (3.1.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->kubernetes===10.0.0-snapshot) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->kubernetes===10.0.0-snapshot) (2.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.6/site-packages (from requests-oauthlib->kubernetes===10.0.0-snapshot) (3.0.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes===10.0.0-snapshot) (0.4.5)\n",
      "Building wheels for collected packages: kubernetes\n",
      "  Building wheel for kubernetes (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-7451k5e1/wheels/09/2b/27/9e7dcb3e2ee8d46375251ebddc69cf5d57b53d00ad949f58ca\n",
      "Successfully built kubernetes\n",
      "\u001b[31mError checking for conflicts.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2897, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2691, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2888, in _parsed_pkg_info\n",
      "    return self._pkg_info\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2691, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _pkg_info\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 503, in _warn_about_conflicts\n",
      "    package_set, _dep_info = check_install_conflicts(to_install)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/operations/check.py\", line 108, in check_install_conflicts\n",
      "    package_set, _ = create_package_set_from_installed()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/operations/check.py\", line 47, in create_package_set_from_installed\n",
      "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2635, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2899, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2908, in _compute_dependencies\n",
      "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2890, in _parsed_pkg_info\n",
      "    metadata = self.get_metadata(self.PKG_INFO)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1410, in get_metadata\n",
      "    value = self._get(self._fn(self.egg_info, name))\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1522, in _get\n",
      "    with open(path, 'rb') as stream:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/jovyan/.local/lib/python3.6/site-packages/kfp-0.1.32.dist-info/METADATA'\u001b[0m\n",
      "Installing collected packages: kubernetes\n",
      "  Found existing installation: kubernetes 9.0.0\n",
      "    Uninstalling kubernetes-9.0.0:\n",
      "      Successfully uninstalled kubernetes-9.0.0\n",
      "Successfully installed kubernetes-10.0.0-snapshot\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml\n",
    "!pip install git+https://github.com/kubernetes-client/python.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from requests import get\n",
    "from lxml import etree\n",
    "from time import sleep\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeMailArchives(mailingList: str, year: int, month: int):\n",
    "    baseUrl = \"http://mail-archives.apache.org/mod_mbox/%s/%s.mbox/ajax/\" % (mailingList, datetime(year,month,1).strftime(\"%Y%m\"))\n",
    "    r = get(baseUrl + \"thread?0\")\n",
    "    utf8_parser = etree.XMLParser(encoding='utf-8')\n",
    "    root = etree.fromstring(r.text.replace('encoding=\"UTF-8\"', \"\"),  parser=utf8_parser)\n",
    "    output = []\n",
    "    for message in root.xpath(\"//message\"):\n",
    "        _id = message.get(\"id\")\n",
    "        linked = message.get(\"linked\")\n",
    "        depth = message.get(\"depth\")\n",
    "        fr = message.xpath(\"from\")[0].text\n",
    "        dt = message.xpath(\"date\")[0].text ## todo convert to date\n",
    "        subject = message.xpath(\"subject\")[0].text\n",
    "        r2 = get(baseUrl + _id)\n",
    "        bodyRoot = etree.fromstring(r2.text.replace('encoding=\"UTF-8\"', \"\"),  parser=utf8_parser)\n",
    "        body = bodyRoot.xpath(\"//contents\")[0].text\n",
    "        record = {\n",
    "            \"id\"        : _id,\n",
    "            \"linked\"    : linked,\n",
    "            \"depth\"     : depth,\n",
    "            \"from\"      : fr,\n",
    "            \"dt\"        : dt,\n",
    "            \"subject\"   : subject,\n",
    "            \"body\"      : body\n",
    "        }\n",
    "        output.append(record)\n",
    "        sleep(0.1)\n",
    "    return output\n",
    "\n",
    "\n",
    "def extract_links(body):\n",
    "    link_regex_str = r'(http(|s)://(.*?))([\\s\\n]|$)'\n",
    "    itr = re.finditer(link_regex_str, body, re.MULTILINE)\n",
    "    return list(map(lambda elem: elem.group(1), itr))\n",
    "\n",
    "def extract_domains(links):\n",
    "    from urllib.parse import urlparse\n",
    "    def extract_domain(link):\n",
    "        try:\n",
    "            nloc = urlparse(link).netloc\n",
    "            # We want to drop www and any extra spaces wtf nloc on the spaces.\n",
    "            regex_str = r'^(www\\.|)(.*?)\\s*$'\n",
    "            match = re.search(regex_str, nloc)\n",
    "            return match.group(2)\n",
    "        except:\n",
    "            return None\n",
    "    return list(map(extract_domain, links))\n",
    "\n",
    "def contains_python_stack_trace(body):\n",
    "    return \"Traceback (most recent call last)\" in body\n",
    "\n",
    "def contains_probably_java_stack_trace(body):\n",
    "    # Look for something based on regex\n",
    "    # Tried https://stackoverflow.com/questions/20609134/regular-expression-optional-multiline-java-stacktrace - more msg looking\n",
    "    # Tried https://stackoverflow.com/questions/3814327/regular-expression-to-parse-a-log-file-and-find-stacktraces\n",
    "    # Yes the compile is per call, but it's cached so w/e\n",
    "    import re\n",
    "    stack_regex_str = r'^\\s*(.+Exception.*):\\n(.*\\n){0,3}?(\\s+at\\s+.*\\(.*\\))+'\n",
    "    match = re.search(stack_regex_str, body, re.MULTILINE)\n",
    "    return match is not None\n",
    "\n",
    "def contains_exception_in_task(body):\n",
    "    # Look for a line along the lines of ERROR Executor: Exception in task\n",
    "    return \"ERROR Executor: Exception in task\" in body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datesToScrape =  [(2019, i) for i in range(1,10)] # + \\\n",
    "#                 [(2018, i) for i in range(1,13)] + \\\n",
    "#                 [(2017, i) for i in range(1,13)] + \n",
    "\n",
    "records = []\n",
    "## todo, go back further\n",
    "for y,m in datesToScrape:\n",
    "    print(m,\"-\",y)\n",
    "    records += scrapeMailArchives(\"spark-dev\", y, m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(records)\n",
    "df['links'] = df['body'].apply(extract_links)\n",
    "df['containsPythonStackTrace'] = df['body'].apply(contains_python_stack_trace)\n",
    "df['containsJavaStackTrace'] = df['body'].apply(contains_probably_java_stack_trace)\n",
    "df['containsExceptionInTaskBody'] = df['body'].apply(contains_exception_in_task)\n",
    "\n",
    "df['domains'] = df['links'].apply(extract_domains)\n",
    "df['isThreadStart'] = df['depth'] == '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "bodyV = TfidfVectorizer()\n",
    "# bodyV = TfidfVectorizer(max_features=10000) #if we cared about making this 1:1 w holden's code.\n",
    "bodyFeatures = bodyV.fit_transform(df['body'])\n",
    "\n",
    "domainV = TfidfVectorizer()\n",
    "# domainV = TfidfVectorizer(max_features=100)\n",
    "\n",
    "## A couple of \"None\" domains really screwed the pooch on this one. Also, no lists just space seperated domains.\n",
    "def makeDomainsAList(d):\n",
    "    return ' '.join([a for a in d if not a is None])\n",
    "\n",
    "domainFeatures = domainV.fit_transform(df['domains'].apply(makeDomainsAList))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "data = hstack([csr_matrix(df[['containsPythonStackTrace', 'containsJavaStackTrace', 'containsExceptionInTaskBody', 'isThreadStart']].to_numpy()),\n",
    "                             bodyFeatures,\n",
    "                            domainFeatures])\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.1)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42).fit(train)\n",
    "train_pred = kmeans.predict(train)\n",
    "test_pred = kmeans.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's build some Pipelines!\n",
    "\n",
    "Rawkintrevo is a rawkin dummy so check this link early and often: \n",
    "https://www.kubeflow.org/docs/pipelines/sdk/sdk-overview/#standard-component-in-app\n",
    "\n",
    "When I'm learning something new, I like to start with the dead simple example first and then \"build it up\" from there.  When it comes to composing Pipelines with Python, the dead simplest route are \"Lightweight Python Functions\" (link), and the dead simplest function is an echo. So here we're going to make our \"Hello World\" which is actually a numerical echo pipeline. That is to say, you put a number in, it spits the number out.\n",
    "\n",
    "Let's start by importing `kfp` and defining our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install 'kfp' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "def deadSimpleIntEchoFn(i: int) -> int:\n",
    "    return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, nothing too crazy there, amirigh? Notice we typed everything. You don't _have_ to as far as I know, but really that's just good practice. Many a bug have been introduced by not strongly typing variables. I'm sure some Python-ados are cussing up a storm right now, and to them I say- go back to R, the adults are talking over here. \n",
    "\n",
    "Next step, we want to wrap our function `deadSimpleIntEchoFn` into a Kubeflow Pipelin Operation.  There's a nice little method to do this called `kfp.components.func_to_container_op`. This will return a factory function with a strongly typed signature. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleStronglyTypedFunction = kfp.components.func_to_container_op(deadSimpleIntEchoFn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create a pipeline in the next step, the factory will construct a `ContainerOp`, which will run the original function (`deadSimpleIntEchoFn`) in a container. To prove that, check this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kfp.dsl._container_op.ContainerOp"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = simpleStronglyTypedFunction(1)\n",
    "type(foo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, I know that seems trivial, I myself am nearly bored to tears. But this will be important when we start trying to mount data volumes (of y'know, training data, and other stuff), in the second merry-go-round.\n",
    "\n",
    "For now, let's finish off this patronizingly simple example by defining and compiling our Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "  name='Dead Simple 1',\n",
    "  description='This is for all intents and purposes and echo pipeline. We take a number in, we spit it back out.'\n",
    ")\n",
    "def echo_pipeline(param_1: kfp.dsl.PipelineParam):\n",
    "  my_step = simpleStronglyTypedFunction(i= param_1)\n",
    "\n",
    "kfp.compiler.Compiler().compile(echo_pipeline,  \n",
    "  'echo-pipeline.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now to run this pipeline, you can go one of two ways. You can run it via code, or you can download `echo-pipeline.zip` go over to the Pipelines WebUI, upload it, do experiments, etc. I get paid by the page over here, so guess which one I'm going to take screen shots of? \n",
    "\n",
    "[Screenshots of rt click / download / uplaod ]\n",
    "\n",
    "Jk, in the next example I'll show how to execute it with code too, but it's not nearly as pretty. I feel like people making these back end tools really never spend enough time on making the WebUI look good. Two notable exceptions are Apache Flink, and Kubeflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A slightly less dead simple example\n",
    "\n",
    "In this we're going ot pull down some data from the internet.  IRL you would probably want to connect to a database or a stream or whatever the cool kids are doing by the time you read this. \n",
    "\n",
    "First we'll declare our data fetching algorithm, which takes an Apache Email List, and a year and will get first a list of all the messages, then iterarte through and download each message. Note two things- it will get at MOST one year of data, and it sleeps in between calls. Don't be a jerk- if you want to try something on bigger data, got get your own set, the Apache Software Foundation is a charity- so don't abuse their mail archives just because. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data() -> bool:\n",
    "    ## packages_to_install should cover this, but seems to be missing? so we have to hack\n",
    "    from pip._internal import main as pipmain\n",
    "    pipmain(['install', 'lxml', 'requests'])\n",
    "    \n",
    "    # / hacks\n",
    "    \n",
    "    from datetime import datetime\n",
    "    from lxml import etree\n",
    "    from requests import get\n",
    "    from time import sleep\n",
    "    \n",
    "    import json\n",
    "    \n",
    "    def scrapeMailArchives(mailingList: str, year: int, month: int):\n",
    "        baseUrl = \"http://mail-archives.apache.org/mod_mbox/%s/%s.mbox/ajax/\" % (mailingList, datetime(year,month,1).strftime(\"%Y%m\"))\n",
    "        r = get(baseUrl + \"thread?0\")\n",
    "        utf8_parser = etree.XMLParser(encoding='utf-8')\n",
    "        root = etree.fromstring(r.text.replace('encoding=\"UTF-8\"', \"\"),  parser=utf8_parser)\n",
    "        output = []\n",
    "        for message in root.xpath(\"//message\"):\n",
    "            _id = message.get(\"id\")\n",
    "            linked = message.get(\"linked\")\n",
    "            depth = message.get(\"depth\")\n",
    "            fr = message.xpath(\"from\")[0].text\n",
    "            dt = message.xpath(\"date\")[0].text ## todo convert to date\n",
    "            subject = message.xpath(\"subject\")[0].text\n",
    "            r2 = get(baseUrl + _id)\n",
    "            bodyRoot = etree.fromstring(r2.text.replace('encoding=\"UTF-8\"', \"\"),  parser=utf8_parser)\n",
    "            body = bodyRoot.xpath(\"//contents\")[0].text\n",
    "            record = {\n",
    "                \"id\"        : _id,\n",
    "                \"linked\"    : linked,\n",
    "                \"depth\"     : depth,\n",
    "                \"from\"      : fr,\n",
    "                \"dt\"        : dt,\n",
    "                \"subject\"   : subject,\n",
    "                \"body\"      : body\n",
    "            }\n",
    "            output.append(record)\n",
    "            sleep(0.1)\n",
    "            \n",
    "        return output\n",
    "\n",
    "    datesToScrape =  [(2019, i) for i in range(1,2)] # + \\\n",
    "    #                 [(2018, i) for i in range(1,13)] + \\\n",
    "    #                 [(2017, i) for i in range(1,13)] + \n",
    "\n",
    "    records = []\n",
    "    ## todo, go back further\n",
    "    for y,m in datesToScrape:\n",
    "        print(m,\"-\",y)\n",
    "        records += scrapeMailArchives(\"spark-dev\", y, m)\n",
    "    import os\n",
    "    output_path = '/data_processing/data.json'\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(records, f)\n",
    "    \n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing Witchcraft\n",
    "\n",
    "Remember earlier when we did that big (and arguably pointless) classification of emails from the Apache Spark mailing list? OK, now we're going to do it again, as a \"lightweight\" Python function in a Kubeflow Pipeline.  I hope the irony of the term \"lightweight\" isn't lost on anyone, because this is pretty blatent abuse of something that was originally presented for conveinience. \n",
    "\n",
    "First note, all of the imports and declarations of helper functions MUST be with in the \"ligthweight\" function.  This train wreck even has me upgrading `pandas`.  One could argue (and they would probably be correct) that I have two steps here- feature prep and ML, and as such I should split them. I would say that's fair, but I choose not to do so at this time.  Perhaps in some scripts later on?\n",
    "\n",
    "As has been pointed out so many times before, we assume the reader either arleady understands what is going on with the KMeans clustering, or better yet, doesn't even care. I won't be digging into that right now. What I will point out- and maybe as a note to the editor, the model that is finally saved really ought to be persisted somewhere.  If the model isn't saved, then this basically pointless pipeline, is truly pointless. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make sure we can read that data in the next step (before we write a big complicated model to do whatever torture to it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_witchcraft(fn_input: bool):\n",
    "    \n",
    "    from pip._internal import main as pipmain\n",
    "    pipmain(['install', '--upgrade', 'pandas']) # df.to_numpy is new in 0.24 which isolder than whatever is shipped here\n",
    "    \n",
    "    import json\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(\"loading records...\")\n",
    "    with open('/data_processing/data.json', 'r') as f:\n",
    "        records = json.load(f)\n",
    "    print(\"records loaded\")\n",
    "    \n",
    "    \n",
    "    ## Note: \"Lightweight\" Python Fns mean helper code must be inside the fn. (Bad Form)\n",
    "    def extract_links(body):\n",
    "        link_regex_str = r'(http(|s)://(.*?))([\\s\\n]|$)'\n",
    "        itr = re.finditer(link_regex_str, body, re.MULTILINE)\n",
    "        return list(map(lambda elem: elem.group(1), itr))\n",
    "\n",
    "    def extract_domains(links):\n",
    "        from urllib.parse import urlparse\n",
    "        def extract_domain(link):\n",
    "            try:\n",
    "                nloc = urlparse(link).netloc\n",
    "                # We want to drop www and any extra spaces wtf nloc on the spaces.\n",
    "                regex_str = r'^(www\\.|)(.*?)\\s*$'\n",
    "                match = re.search(regex_str, nloc)\n",
    "                return match.group(2)\n",
    "            except:\n",
    "                return None\n",
    "        return list(map(extract_domain, links))\n",
    "\n",
    "    def contains_python_stack_trace(body):\n",
    "        return \"Traceback (most recent call last)\" in body\n",
    "\n",
    "    def contains_probably_java_stack_trace(body):\n",
    "        # Look for something based on regex\n",
    "        # Tried https://stackoverflow.com/questions/20609134/regular-expression-optional-multiline-java-stacktrace - more msg looking\n",
    "        # Tried https://stackoverflow.com/questions/3814327/regular-expression-to-parse-a-log-file-and-find-stacktraces\n",
    "        # Yes the compile is per call, but it's cached so w/e\n",
    "        import re\n",
    "        stack_regex_str = r'^\\s*(.+Exception.*):\\n(.*\\n){0,3}?(\\s+at\\s+.*\\(.*\\))+'\n",
    "        match = re.search(stack_regex_str, body, re.MULTILINE)\n",
    "        return match is not None\n",
    "\n",
    "    def contains_exception_in_task(body):\n",
    "        # Look for a line along the lines of ERROR Executor: Exception in task\n",
    "        return \"ERROR Executor: Exception in task\" in body\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    print(df.shape)\n",
    "    df['links'] = df['body'].apply(extract_links)\n",
    "    df['containsPythonStackTrace'] = df['body'].apply(contains_python_stack_trace)\n",
    "    df['containsJavaStackTrace'] = df['body'].apply(contains_probably_java_stack_trace)\n",
    "    df['containsExceptionInTaskBody'] = df['body'].apply(contains_exception_in_task)\n",
    "\n",
    "    df['domains'] = df['links'].apply(extract_domains)\n",
    "    df['isThreadStart'] = df['depth'] == '0'\n",
    "    \n",
    "    # Arguably, you could split building the dataset away from the actual witchcraft.\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    bodyV = TfidfVectorizer()\n",
    "    # bodyV = TfidfVectorizer(max_features=10000) #if we cared about making this 1:1 w holden's code.\n",
    "    bodyFeatures = bodyV.fit_transform(df['body'])\n",
    "\n",
    "    domainV = TfidfVectorizer()\n",
    "    # domainV = TfidfVectorizer(max_features=100)\n",
    "\n",
    "    ## A couple of \"None\" domains really screwed the pooch on this one. Also, no lists just space seperated domains.\n",
    "    def makeDomainsAList(d):\n",
    "        return ' '.join([a for a in d if not a is None])\n",
    "\n",
    "    domainFeatures = domainV.fit_transform(df['domains'].apply(makeDomainsAList))\n",
    "\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "    data = hstack([csr_matrix(df[['containsPythonStackTrace', 'containsJavaStackTrace', 'containsExceptionInTaskBody', 'isThreadStart']].to_numpy()),\n",
    "                                 bodyFeatures,\n",
    "                                domainFeatures])\n",
    "\n",
    "\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    train, test = train_test_split(data, test_size=0.1)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42).fit(train)\n",
    "    train_pred = kmeans.predict(train)\n",
    "    test_pred = kmeans.predict(test)\n",
    "    print(test_pred)\n",
    "    # TODO: Dump the model somewhere you can use it later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The Kubeflow Bit.\n",
    "\n",
    "OK, now you've seen a fairly exhuastive demonstration of antipatterns in programming, let's get down to the business of this book and actually create our Kubeflow Pipeline.  First you will notice we have to call in `k8s_client`.  My version of (working) Kubeflow is old and uses a somewhat antiquated method for attatching volumes to `ContainerOps`.  You should recall `ContainrOps` from the previous chapter, but here I will say, they are actually created not at \n",
    "```\n",
    "witchcraft_fn =...\n",
    "```\n",
    "\n",
    "But at \n",
    "```\n",
    "step1 = ...\n",
    "step2 = ...\n",
    "```\n",
    "\n",
    "Which is why it is there, we are adding the `add_volume` and `add_volume_mount`. \n",
    "\n",
    "Obviously PVCs are only one of many ways to get data into our containers, and honestly may not be the _best_ way.  However, we often may \"get\" our data in the first step and then need to hand it, in various stages, from one component to the next, and in these cases, `PersistentVolumeClaims` work exceedingly well, and so they are presented as an example. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client as k8s_client\n",
    "\n",
    "download_data_fn = kfp.components.func_to_container_op(download_data)\n",
    "witchcraft_fn = kfp.components.func_to_container_op(do_witchcraft)\n",
    "\n",
    "@kfp.dsl.pipeline(\n",
    "  name='Simple1',\n",
    "  description='Simple1'\n",
    ")\n",
    "def my_pipeline2():\n",
    "    ## Note this is OLD code, cleaner way to do it now- revisit when you have 0.7 up\n",
    "    step1 = download_data_fn().add_volume(k8s_client.V1Volume(name='data-processing',\n",
    "                                                                                  host_path=k8s_client.V1HostPathVolumeSource(path='/data_processing'))) \\\n",
    "                            .add_volume_mount(k8s_client.V1VolumeMount(\n",
    "                                          mount_path='/data_processing',\n",
    "                                          name='data-processing'))\n",
    "    step2 = witchcraft_fn(fn_input= step1.output).add_volume(k8s_client.V1Volume(name='data-processing',\n",
    "                                                                                  host_path=k8s_client.V1HostPathVolumeSource(path='/data_processing'))) \\\n",
    "                            .add_volume_mount(k8s_client.V1VolumeMount(\n",
    "                                          mount_path='/data_processing',\n",
    "                                          name='data-processing'))\n",
    "\n",
    "kfp.compiler.Compiler().compile(my_pipeline2, 'my-pipeline6.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that, we've done it. We've created a Kubeflow Pipeline.\n",
    "\n",
    "So let's take a moment to step back and think, \"what in the crazy-town-heck is going on here?!\".  A valid question, and well spotted.  Each \"Step\" is going to be creating a container.  Maybe I should have noted that earlier when talking about attatching volumes, beacuse if you thougth I was doing that to a function, you'd probably think me quite insane. \n",
    "\n",
    "But, if you follow this code, and create this pipeline, download it and run it, you will see each \"step\" as a seperate container, downloading data, saving it to a `PVC` then passing some parameters to a next container, which also will load the `PVC`, etc. etc.  \n",
    "\n",
    "### Using Python to Create Containers, but not like a crazy person\n",
    "\n",
    "For completeness, let's last explore how to do all of these things like not a psycopath. \n",
    "\n",
    "The trick for the most part is to create a function that returns a `kfp.dsl.ContainerOp`.  This will point to an image, note the volumes that need to be mounted, and a number of other things. I've heard told people don't always just like creating absurdly large and fat functions to do everything in real life, so I leave this hear as an aside in case the reader is interested in it.  It's alsow worth noting that adding the `@kfp.dsl.component` annotation instructs teh Kubeflow compiler to turn on static typce checking. \n",
    "\n",
    "```\n",
    "@kfp.dsl.component\n",
    "def my_component(my_param):\n",
    "  ...\n",
    "  return kfp.dsl.ContainerOp(\n",
    "    name='My component name',\n",
    "    image='gcr.io/path/to/container/image'\n",
    "  )\n",
    "```\n",
    "\n",
    "Finally, when it comes to incorporating these components into pipelines, you would do something like this:\n",
    "\n",
    "```\n",
    "@kfp.dsl.pipeline(\n",
    "  name='My pipeline',\n",
    "  description='My machine learning pipeline'\n",
    ")\n",
    "def my_pipeline(param_1: PipelineParam, param_2: PipelineParam):\n",
    "  my_step = my_component(my_param='a')\n",
    "```\n",
    "\n",
    "Which should look exceedingly familiar as we did something very similar with our `download_data_fn` and `witchcraft_fn`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
